The first submission I originally planned to submit was from the lab2 take home exercises, using bag of words and Decision tree. However, when running the accuracy metrics, it proved to have a very low score. The next attempted model was Naïve Bayes. This model is described as a “probabilistic classifier” and does not require extensive training data. As it is simple and easy to implement, I assumed the google colab notebook would not have resource issues when running the model. 
For the Data preparation I cleaned and standardized the text. This included removing stop words, punctuation, extra spaces, etc. and changing all words to lower case. Then saved it as a pickle file so it can quickly be loaded for other attempts. Since we had already used bag of words in the lab exercise, I re-used that code. This provided a decent accuracy score. 
Then I decided to implement TF-IDF instead. This did perform slightly better than BoW. Ultimately, this was my best submission in the competition with a public score of 0.46185 and Private score of 0.44688. Interesting enough, the submission using logistic Regression had originally scored lower in the public score (0.46142) but did better in the Private score with 0.44728. It is not a significant difference but unexpected. 
 
My Next attempts never made it to submission as I did not have the necessary resources. Each try would run for 6+ hours but ultimately crash. The first change was to use Word2Vec instead of TF-IDF. Word2Vec technique would learn the contextual relationship of the text data. I attempted to pair it with Random Forest algorithm. However, the system never finished predicting the test label data as it ran out of available memory. 
The last model I researched was RoBERTa. It is similar to BERT in the sense that it uses bidirectional context to capture the relationships between words in a sentence. It produces more robust representation due to its use of dynamic masking where the pattern is different for each training epoch. But due to resource limitations I was unable to complete training the model.
